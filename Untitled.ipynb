{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bb4a1cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "chunk_overlap_ratio must be a float between 0. and 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 40\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mresponse\n\u001b[1;32m     35\u001b[0m iface \u001b[39m=\u001b[39m gr\u001b[39m.\u001b[39mInterface(fn\u001b[39m=\u001b[39mchatbot,\n\u001b[1;32m     36\u001b[0m                      inputs\u001b[39m=\u001b[39mgr\u001b[39m.\u001b[39mcomponents\u001b[39m.\u001b[39mTextbox(lines\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEnter your text\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     37\u001b[0m                      outputs\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m                      title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCustom-trained AI Chatbot\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m index \u001b[39m=\u001b[39m construct_index(\u001b[39m\"\u001b[39;49m\u001b[39mdocs\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     41\u001b[0m iface\u001b[39m.\u001b[39mlaunch(share\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[20], line 18\u001b[0m, in \u001b[0;36mconstruct_index\u001b[0;34m(directory_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m chunk_size_limit \u001b[39m=\u001b[39m \u001b[39m600\u001b[39m\n\u001b[1;32m     15\u001b[0m chunk_overlap_ratio\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m\n\u001b[0;32m---> 18\u001b[0m prompt_helper \u001b[39m=\u001b[39m PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit\u001b[39m=\u001b[39;49mchunk_size_limit)\n\u001b[1;32m     20\u001b[0m llm_predictor \u001b[39m=\u001b[39m LLMPredictor(llm\u001b[39m=\u001b[39mChatOpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m, max_tokens\u001b[39m=\u001b[39mnum_outputs))\n\u001b[1;32m     22\u001b[0m documents \u001b[39m=\u001b[39m SimpleDirectoryReader(directory_path)\u001b[39m.\u001b[39mload_data()\n",
      "File \u001b[0;32m~/Desktop/AI/venv/lib/python3.10/site-packages/llama_index/indices/prompt_helper.py:65\u001b[0m, in \u001b[0;36mPromptHelper.__init__\u001b[0;34m(self, context_window, num_output, chunk_overlap_ratio, chunk_size_limit, tokenizer, separator)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_overlap_ratio \u001b[39m=\u001b[39m chunk_overlap_ratio\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_overlap_ratio \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_overlap_ratio \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mchunk_overlap_ratio must be a float between 0. and 1.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_size_limit \u001b[39m=\u001b[39m chunk_size_limit\n\u001b[1;32m     68\u001b[0m \u001b[39m# TODO: make configurable\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: chunk_overlap_ratio must be a float between 0. and 1."
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader, GPTListIndex, GPTVectorStoreIndex, LLMPredictor, PromptHelper, ServiceContext, StorageContext, load_index_from_storage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import gradio as gr\n",
    "import sys\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai_api_key = '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'\n",
    "\n",
    "def construct_index(directory_path):\n",
    "    max_input_size = 4096\n",
    "    num_outputs = 512\n",
    "    max_chunk_overlap = 20\n",
    "    chunk_size_limit = 600\n",
    "    chunk_overlap_ratio=0.1\n",
    "\n",
    "\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n",
    "\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "\n",
    "    index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper,chunk_overlap_ratio=chunk_overlap_ratio)\n",
    "\n",
    "    index.save_to_disk('index.json')\n",
    "\n",
    "    return index\n",
    "\n",
    "def chatbot(input_text):\n",
    "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
    "    response = index.query(input_text, response_mode=\"compact\")\n",
    "    return response.response\n",
    "\n",
    "iface = gr.Interface(fn=chatbot,\n",
    "                     inputs=gr.components.Textbox(lines=7, label=\"Enter your text\"),\n",
    "                     outputs=\"text\",\n",
    "                     title=\"Custom-trained AI Chatbot\")\n",
    "\n",
    "index = construct_index(\"docs\")\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f496bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid OpenAI API key.\nAPI key should be of the format: \"sk-\" followed by 48 alphanumeric characters.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mresponse\n\u001b[1;32m     33\u001b[0m iface \u001b[39m=\u001b[39m gr\u001b[39m.\u001b[39mInterface(fn\u001b[39m=\u001b[39mchatbot,\n\u001b[1;32m     34\u001b[0m                      inputs\u001b[39m=\u001b[39mgr\u001b[39m.\u001b[39mcomponents\u001b[39m.\u001b[39mTextbox(lines\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEnter your text\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     35\u001b[0m                      outputs\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m                      title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCustom-trained AI Chatbot\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m index \u001b[39m=\u001b[39m construct_index(\u001b[39m\"\u001b[39;49m\u001b[39mdocs\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     39\u001b[0m iface\u001b[39m.\u001b[39mlaunch(share\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mconstruct_index\u001b[0;34m(directory_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m llm_predictor \u001b[39m=\u001b[39m LLMPredictor(llm\u001b[39m=\u001b[39mChatOpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m, max_tokens\u001b[39m=\u001b[39mnum_outputs))\n\u001b[1;32m     20\u001b[0m documents \u001b[39m=\u001b[39m SimpleDirectoryReader(directory_path)\u001b[39m.\u001b[39mload_data()\n\u001b[0;32m---> 22\u001b[0m index \u001b[39m=\u001b[39m GPTVectorStoreIndex(documents, llm_predictor\u001b[39m=\u001b[39;49mllm_predictor, prompt_helper\u001b[39m=\u001b[39;49mprompt_helper, chunk_overlap_ratio\u001b[39m=\u001b[39;49mchunk_overlap_ratio)\n\u001b[1;32m     24\u001b[0m index\u001b[39m.\u001b[39msave_to_disk(\u001b[39m'\u001b[39m\u001b[39mindex.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m index\n",
      "File \u001b[0;32m~/Desktop/AI/venv/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py:46\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_async \u001b[39m=\u001b[39m use_async\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_store_nodes_override \u001b[39m=\u001b[39m store_nodes_override\n\u001b[0;32m---> 46\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     47\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m     48\u001b[0m     index_struct\u001b[39m=\u001b[39;49mindex_struct,\n\u001b[1;32m     49\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[1;32m     50\u001b[0m     storage_context\u001b[39m=\u001b[39;49mstorage_context,\n\u001b[1;32m     51\u001b[0m     show_progress\u001b[39m=\u001b[39;49mshow_progress,\n\u001b[1;32m     52\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     53\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/AI/venv/lib/python3.10/site-packages/llama_index/indices/base.py:61\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, index_struct, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnodes must be a list of Node objects.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_context \u001b[39m=\u001b[39m service_context \u001b[39mor\u001b[39;00m ServiceContext\u001b[39m.\u001b[39;49mfrom_defaults()\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage_context \u001b[39m=\u001b[39m storage_context \u001b[39mor\u001b[39;00m StorageContext\u001b[39m.\u001b[39mfrom_defaults()\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_docstore \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage_context\u001b[39m.\u001b[39mdocstore\n",
      "File \u001b[0;32m~/Desktop/AI/venv/lib/python3.10/site-packages/llama_index/indices/service_context.py:159\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot specify both llm and llm_predictor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m     llm_predictor \u001b[39m=\u001b[39m LLMPredictor(llm\u001b[39m=\u001b[39mllm)\n\u001b[0;32m--> 159\u001b[0m llm_predictor \u001b[39m=\u001b[39m llm_predictor \u001b[39mor\u001b[39;00m LLMPredictor()\n\u001b[1;32m    160\u001b[0m llm_predictor\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager\n\u001b[1;32m    162\u001b[0m \u001b[39m# NOTE: the embed_model isn't used in all indices\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI/venv/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:70\u001b[0m, in \u001b[0;36mLLMPredictor.__init__\u001b[0;34m(self, llm, callback_manager)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     66\u001b[0m     llm: Optional[LLMType] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     67\u001b[0m     callback_manager: Optional[CallbackManager] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     68\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm \u001b[39m=\u001b[39m resolve_llm(llm)\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager \u001b[39mor\u001b[39;00m CallbackManager([])\n",
      "File \u001b[0;32m~/Desktop/AI/venv/lib/python3.10/site-packages/llama_index/llms/utils.py:17\u001b[0m, in \u001b[0;36mresolve_llm\u001b[0;34m(llm)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(llm, BaseLanguageModel):\n\u001b[1;32m     14\u001b[0m     \u001b[39m# NOTE: if it's a langchain model, wrap it in a LangChainLLM\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m LangChainLLM(llm\u001b[39m=\u001b[39mllm)\n\u001b[0;32m---> 17\u001b[0m \u001b[39mreturn\u001b[39;00m llm \u001b[39mor\u001b[39;00m OpenAI()\n",
      "File \u001b[0;32m~/Desktop/AI/venv/lib/python3.10/site-packages/llama_index/llms/openai.py:46\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     validate_openai_api_key(\n\u001b[1;32m     47\u001b[0m         kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mapi_key\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mapi_type\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/AI/venv/lib/python3.10/site-packages/llama_index/llms/openai_utils.py:272\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[0;34m(api_key, api_type)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n\u001b[1;32m    269\u001b[0m \u001b[39melif\u001b[39;00m openai_api_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mopen_ai\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m OPENAI_API_KEY_FORMAT\u001b[39m.\u001b[39msearch(\n\u001b[1;32m    270\u001b[0m     openai_api_key\n\u001b[1;32m    271\u001b[0m ):\n\u001b[0;32m--> 272\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(INVALID_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid OpenAI API key.\nAPI key should be of the format: \"sk-\" followed by 48 alphanumeric characters.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader, GPTListIndex, GPTVectorStoreIndex, LLMPredictor, PromptHelper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import gradio as gr\n",
    "import sys\n",
    "import os\n",
    "import openai\n",
    "\n",
    "OPENAI_API_KEY ='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'\n",
    "\n",
    "def construct_index(directory_path):\n",
    "    max_input_size = 4096\n",
    "    num_outputs = 512\n",
    "    chunk_overlap_ratio = 0.2  # Correct the value here: it should be between 0.0 and 1.0\n",
    "    chunk_size_limit = 600\n",
    "\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, chunk_overlap_ratio, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n",
    "\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "\n",
    "    index = GPTVectorStoreIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper, chunk_overlap_ratio=chunk_overlap_ratio)\n",
    "\n",
    "    index.save_to_disk('index.json')\n",
    "\n",
    "    return index\n",
    "\n",
    "def chatbot(input_text):\n",
    "    index = GPTVectorStoreIndex.load_from_disk('index.json')\n",
    "    response = index.query(input_text, response_mode=\"compact\")\n",
    "    return response.response\n",
    "\n",
    "iface = gr.Interface(fn=chatbot,\n",
    "                     inputs=gr.components.Textbox(lines=7, label=\"Enter your text\"),\n",
    "                     outputs=\"text\",\n",
    "                     title=\"Custom-trained AI Chatbot\")\n",
    "\n",
    "index = construct_index(\"docs\")\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e5f4eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('eEtTxWPftjvIIN3DkuWyT3BlbkFJzg0T5cckD1XJ9I902Yol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a311f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
